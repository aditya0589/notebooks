{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMeygt8/hoRvSfTARCBj7Re"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **NLP02  Text Representation and Engineering**"],"metadata":{"id":"59o8OGTTOh9k"}},{"cell_type":"markdown","source":["In order to perform any NLP application, you would need to pass the textual data into a machine learning or deep learning algorithm. However, most of these models do not understand textual data. They only work with numbers and vectors.\n","\n","Therefore it is important to first convert these text into a suitable numerical format.\n","\n","There are various ways in which this can be done. We will explore them in this notebook"],"metadata":{"id":"HJBdoGPZXfQr"}},{"cell_type":"markdown","source":["## **One Hot Encoding**"],"metadata":{"id":"H_QhhBaMZuEq"}},{"cell_type":"markdown","source":["One-hot encoding is a fundamental technique used in Natural Language Processing (NLP) to convert text data into numerical representations that machine learning models can process. Since models cannot directly understand raw text, words must be transformed into numeric vectors.\n","\n","In NLP, one-hot encoding represents each distinct word in the vocabulary as a binary vector, where:\n","\n","The length of the vector equals the total number of unique words in the corpus (vocabulary size).\n","\n","Each word is assigned a unique index.\n","\n","In the vector, only one position is 1 (corresponding to the wordâ€™s index), and all other positions are 0.\n","\n","\n","**Why One-Hot Encoding is Needed in NLP**\n","<br>\n","Natural language contains words, phrases, and sentences that must be converted into numbers so models can perform computations.\n","One-hot encoding allows text to be represented mathematically without implying any order, priority, or similarity between words."],"metadata":{"id":"Lhhe_gEmb7XL"}},{"cell_type":"markdown","source":["example:<br>[\"cat\", \"dog\", \"apple\", \"ball\", \"milk\"]<br>\n","\n","Assign each word an index:\n","\n","cat â†’ 0 <br>\n","dog â†’ 1 <br>\n","apple â†’ 2<br>\n","ball â†’ 3<br>\n","milk â†’ 4<br>\n","\n","Now convert using one-hot encoding:\n","\n","| Word  | One-Hot Vector  |\n","| ----- | --------------- |\n","| cat   | [1, 0, 0, 0, 0] |\n","| dog   | [0, 1, 0, 0, 0] |\n","| apple | [0, 0, 1, 0, 0] |\n","| ball  | [0, 0, 0, 1, 0] |\n","| milk  | [0, 0, 0, 0, 1] |\n","\n","Each word becomes a vector of zeros except one position set to 1.\n","\n"],"metadata":{"id":"cHkWhqdlcFLL"}},{"cell_type":"markdown","source":["One hot encoding is the most intuitive way of representing text and numbers and it highly easy to implement. However, it has many drawbacks and thus is not used in modern NLP applications\n","\n","**Limitations of One hot Encoding:**\n","1. **sparsity of vectors**: Large number of zeroes in the vector makes them very similar and difficult to interpret and process\n","2. **No fixed size of input data**: Most of the times, the input data has no fixed size. Therefore most machine learning algorithms fail to handle such data\n","3. **Out of Vocabulary**: If the data entered by the user has words which are not present in the training vocabulary, the model fails.\n","4. **No capturing of semantic meaning**: The semantic meaning of the words is not captured."],"metadata":{"id":"oTovbLSadMud"}},{"cell_type":"code","source":["paragraph = \"\"\"\n","Natural Language Processing focuses on understanding and generating human language.\n","It enables machines to interpret text and extract meaningful information.\n","NLP is an exciting field with many real-world applications.\n","\"\"\"\n"],"metadata":{"id":"LQIx7OO7OusS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Create the vectorizer\n","vectorizer = CountVectorizer(binary=True)\n","\n","# Fit and transform paragraph\n","one_hot_matrix = vectorizer.fit_transform([paragraph])\n","\n","# Display vocabulary\n","print(\"Vocabulary:\\n\", vectorizer.vocabulary_, \"\\n\")\n","\n","# Display one-hot encoded matrix\n","print(\"One-Hot Encoded Output:\\n\", one_hot_matrix.toarray())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d4OkoOAUe_km","executionInfo":{"status":"ok","timestamp":1765290043701,"user_tz":-330,"elapsed":2920,"user":{"displayName":"6676 Aditya","userId":"06090099644586545117"}},"outputId":"9cd63c9e-7101-4b6a-8840-111e69e26ec3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary:\n"," {'natural': 18, 'language': 14, 'processing': 21, 'focuses': 7, 'on': 20, 'understanding': 25, 'and': 1, 'generating': 8, 'human': 9, 'it': 13, 'enables': 3, 'machines': 15, 'to': 24, 'interpret': 11, 'text': 23, 'extract': 5, 'meaningful': 17, 'information': 10, 'nlp': 19, 'is': 12, 'an': 0, 'exciting': 4, 'field': 6, 'with': 26, 'many': 16, 'real': 22, 'world': 27, 'applications': 2} \n","\n","One-Hot Encoded Output:\n"," [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"]}]},{"cell_type":"markdown","source":["One hot encoding on sentences be like:"],"metadata":{"id":"LPm7hf3gfYSM"}},{"cell_type":"code","source":["import nltk\n","\n","nltk.download('punkt')        # for sentence & word tokenizer\n","nltk.download('punkt_tab')    # sometimes needed in new versions\n","nltk.download('stopwords')    # stopword list\n","nltk.download('wordnet')      # word lemmatizer dictionary\n","nltk.download('omw-1.4')      # lemma language support\n","nltk.download('averaged_perceptron_tagger')  # POS tagging\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Y-0DIUVfo2E","executionInfo":{"status":"ok","timestamp":1765290187283,"user_tz":-330,"elapsed":1437,"user":{"displayName":"6676 Aditya","userId":"06090099644586545117"}},"outputId":"8438cb46-74a4-416e-f28f-f8ad0ba5e093"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["from nltk.tokenize import sent_tokenize\n","import nltk\n","nltk.download('punkt', quiet=True)\n","\n","sentences = sent_tokenize(paragraph)\n","print(sentences)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mf_9IqClfFyF","executionInfo":{"status":"ok","timestamp":1765290190417,"user_tz":-330,"elapsed":117,"user":{"displayName":"6676 Aditya","userId":"06090099644586545117"}},"outputId":"109b49c9-0bea-4313-c417-7978e010ac66"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['\\nNatural Language Processing focuses on understanding and generating human language.', 'It enables machines to interpret text and extract meaningful information.', 'NLP is an exciting field with many real-world applications.']\n"]}]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer = CountVectorizer(binary=True)\n","sentence_ohe = vectorizer.fit_transform(sentences)\n","\n","print(\"Vocabulary:\\n\", vectorizer.vocabulary_, \"\\n\")\n","print(\"One-Hot Encoded Sentence Matrix:\\n\", sentence_ohe.toarray())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iPdT_W_DfbU_","executionInfo":{"status":"ok","timestamp":1765290195075,"user_tz":-330,"elapsed":9,"user":{"displayName":"6676 Aditya","userId":"06090099644586545117"}},"outputId":"f100f1d6-86de-462b-bc1b-e535b88fb300"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary:\n"," {'natural': 18, 'language': 14, 'processing': 21, 'focuses': 7, 'on': 20, 'understanding': 25, 'and': 1, 'generating': 8, 'human': 9, 'it': 13, 'enables': 3, 'machines': 15, 'to': 24, 'interpret': 11, 'text': 23, 'extract': 5, 'meaningful': 17, 'information': 10, 'nlp': 19, 'is': 12, 'an': 0, 'exciting': 4, 'field': 6, 'with': 26, 'many': 16, 'real': 22, 'world': 27, 'applications': 2} \n","\n","One-Hot Encoded Sentence Matrix:\n"," [[0 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0]\n"," [0 1 0 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0]\n"," [1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1]]\n"]}]},{"cell_type":"markdown","source":["## **Bag of Words**"],"metadata":{"id":"bCOM-r2Sfvv9"}},{"cell_type":"markdown","source":["Bag of Words (BoW) is a fundamental text-representation technique used in NLP that converts text documents into numerical feature vectors.\n","It works by counting the occurrences of each word in a document without considering grammar, order, or semantics.\n","The model treats a document as a bag of words â€” meaning only the frequency matters, not the position.\n","\n","Example:<br>\n","**Doc1: \"I love NLP\"**<br>\n","**Doc2: \"I love machine learning\"**\n","\n","Vocabulary:\n","<br>\n","[\"I\", \"love\", \"NLP\", \"machine\", \"learning\"]\n","\n","\n","| Document | I | love | NLP | machine | learning |\n","| -------- | - | ---- | --- | ------- | -------- |\n","| Doc1     | 1 | 1    | 1   | 0       | 0        |\n","| Doc2     | 1 | 1    | 0   | 1       | 1        |\n","\n","\n","\n"],"metadata":{"id":"21PXvQriv_6s"}},{"cell_type":"markdown","source":["## ðŸ“ Geometric Intuition of Bag of Words (BoW)\n","\n","After applying Bag of Words, each document becomes a vector in a high-dimensional space.  \n","If the vocabulary size is \\(V\\), then each document \\(D_i\\) is represented as:\n","\n","$$\n","D_i = [x_1, x_2, x_3, \\dots, x_V]\n","$$\n","\n","where \\(x_k\\) is the count (or binary presence) of the \\(k^{th}\\) word in the vocabulary.\n","\n","---\n","\n","### Vector Space Representation\n","Each document is a point in vector space:\n","\n","$$\n","D_i \\in \\mathbb{R}^V\n","$$\n","\n","Similarity between documents is commonly measured using **Cosine Similarity**, which measures the angle between two vectors.\n","\n","---\n","\n","### Cosine Similarity Formula\n","\n","For two document vectors \\(D_1\\) and \\(D_2\\):\n","\n","$$\n","\\text{CosineSimilarity}(D_1, D_2) = \\frac{D_1 \\cdot D_2}{\\|D_1\\| \\; \\|D_2\\|}\n","$$\n","\n","Dot product:\n","\n","$$\n","D_1 \\cdot D_2 = \\sum_{i=1}^{V} D_1(i) \\cdot D_2(i)\n","$$\n","\n","Norm (magnitude):\n","\n","$$\n","\\|D\\| = \\sqrt{\\sum_{i=1}^{V} D(i)^2}\n","$$\n","\n","---\n","\n","### Example\n","\n","Let:\n","\n","$$\n","D_1 = [1, 1, 1, 0, 0]\n","$$\n","\n","$$\n","D_2 = [1, 1, 0, 1, 1]\n","$$\n","\n","Dot product:\n","\n","$$\n","D_1 \\cdot D_2 = (1)(1)+(1)(1)+(1)(0)+(0)(1)+(0)(1)=2\n","$$\n","\n","Magnitudes:\n","\n","$$\n","\\|D_1\\| = \\sqrt{1^2 + 1^2 + 1^2} = \\sqrt{3}\n","$$\n","\n","$$\n","\\|D_2\\| = \\sqrt{1^2 + 1^2 + 1^2 + 1^2} = \\sqrt{4} = 2\n","$$\n","\n","Cosine similarity:\n","\n","$$\n","\\text{Cosine}(D_1, D_2) = \\frac{2}{\\sqrt{3} \\cdot 2} = \\frac{1}{\\sqrt{3}} \\approx 0.577\n","$$\n","\n","---\n","\n","### Interpretation\n","- Value close to **1** â†’ documents are highly similar  \n","- Value close to **0** â†’ documents are unrelated  \n","- BoW places documents in a high-dimensional space, and the **angle between vectors defines similarity**\n","\n","---\n","\n","### Key Properties\n","\n","$$\n","\\text{Dimensionality} = V\n","$$\n","\n","$$\n","\\text{Most entries are zeros (sparse vectors)}\n","$$\n","\n","$$\n","\\text{Similarity is based on angle between vectors}\n","$$\n"],"metadata":{"id":"-IX02D-CxEwO"}},{"cell_type":"code","source":["# Bag of Words implementation using sklearn\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Example documents\n","documents = [\n","    \"I love natural language processing\",\n","    \"Language processing helps computers understand text\",\n","    \"I love deep learning and machine learning\"\n","]\n","\n","# Initialize the CountVectorizer (Bag of Words)\n","vectorizer = CountVectorizer()\n","\n","# Fit and transform the documents into BoW matrix\n","bow_matrix = vectorizer.fit_transform(documents)\n","\n","# Display vocabulary (word index mapping)\n","print(\"Vocabulary (word -> index):\")\n","print(vectorizer.vocabulary_)\n","print(\"\\n\")\n","\n","# Display Bag of Words matrix in array form\n","print(\"Bag of Words Matrix:\")\n","print(bow_matrix.toarray())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SVVrw3z0frbH","executionInfo":{"status":"ok","timestamp":1765295294225,"user_tz":-330,"elapsed":54,"user":{"displayName":"6676 Aditya","userId":"06090099644586545117"}},"outputId":"2814a195-9d40-4231-b6bc-9b453035d44e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary (word -> index):\n","{'love': 6, 'natural': 8, 'language': 4, 'processing': 9, 'helps': 3, 'computers': 1, 'understand': 11, 'text': 10, 'deep': 2, 'learning': 5, 'and': 0, 'machine': 7}\n","\n","\n","Bag of Words Matrix:\n","[[0 0 0 0 1 0 1 0 1 1 0 0]\n"," [0 1 0 1 1 0 0 0 0 1 1 1]\n"," [1 0 1 0 0 2 1 1 0 0 0 0]]\n"]}]},{"cell_type":"markdown","source":["## **Bag of N-Grams**"],"metadata":{"id":"ZOxIzkA9zRn0"}},{"cell_type":"markdown","source":["**What are N-grams?**\n","\n","N-grams are continuous sequences of **N words** (or tokens) taken from a text.  \n","They help capture **local context**, which single words (unigrams) cannot represent.\n","\n","### Example:\n","For the sentence:\n","<br>\n","I love natural language processing\n","\n","\n","| N-grams | Output |\n","|---------|--------|\n","| 1-gram (Unigram) | I, love, natural, language, processing |\n","| 2-gram (Bigram) | I love, love natural, natural language, language processing |\n","| 3-gram (Trigram) | I love natural, love natural language, natural language processing |\n","\n","---\n","\n","## Why N-grams? â€” Intuition\n","Unigrams treat every word independently â†’ **no context** is captured.  \n","N-grams help model word relationships and semantic dependencies.\n","\n","- **Bigram intuition:** Predicts next word from previous one  \n","  Example: \"machine learning\" occurs often, \"machine pizza\" rarely\n","- **Trigram & higher:** capture phrase-level meaning\n","\n","Thus, N-grams help build better predictive models for language.\n","\n","---\n","\n","## Mathematical Formulation\n","\n","### Probability of a sentence\n","To estimate the probability of a sequence of words:\n","\n","$$\n","P(w_1, w_2, w_3, \\dots, w_T)\n","$$\n","\n","Using the **Chain Rule**:\n","\n","$$\n","P(w_1, w_2, \\dots, w_T) = \\prod_{t=1}^{T} P(w_t \\mid w_1, w_2, \\dots, w_{t-1})\n","$$\n","\n","This is computationally huge, so we apply the **Markov assumption**:\n","\n","$$\n","P(w_t \\mid w_1, \\dots, w_{t-1}) \\approx P(w_t \\mid w_{t-(N-1)}, \\dots, w_{t-1})\n","$$\n","\n","---\n","\n","### Bigram Model\n","$$\n","P(w_1, w_2, \\dots, w_T) \\approx \\prod_{t=1}^{T} P(w_t \\mid w_{t-1})\n","$$\n","\n","Probability estimated using frequency counts:\n","\n","$$\n","P(w_t \\mid w_{t-1}) = \\frac{\\text{Count}(w_{t-1}, w_t)}{\\text{Count}(w_{t-1})}\n","$$\n","\n","---\n","\n","### Trigram Model\n","$$\n","P(w_t \\mid w_{t-1}, w_{t-2}) = \\frac{\\text{Count}(w_{t-2}, w_{t-1}, w_t)}{\\text{Count}(w_{t-2}, w_{t-1})}\n","$$\n","\n","---\n","\n","## Geometric / Vector Intuition\n","N-grams can be represented in a vector space like Bag of Words:\n","\n","- Each **N-gram** becomes a dimension\n","- Each document becomes a vector of **N-gram frequencies**\n","- Similarity between documents can be measured via cosine similarity\n","\n","Example vector representation:\n","\n","$$\n","D = [\\text{count(\"I love\")}, \\text{count(\"love NLP\")}, \\dots ]\n","$$\n","\n","Documents close together in the vector space share similar language patterns.\n","\n","---\n","\n","## Applications of N-grams\n","| Use Case | Example |\n","|-----------|---------|\n","| Language models | Next-word prediction (autocomplete) |\n","| Speech recognition | Predicts likely words |\n","| Machine translation | Translates phrase-by-phrase |\n","| Sentiment analysis | Detecting opinion phrases |\n","| Plagiarism detection | N-gram overlap |\n","| Spell correction | Context-based replacement |\n","\n","---\n","\n","## Limitations\n","| Issue | Description |\n","|--------|------------|\n","| High dimensionality | Vocabulary grows exponentially with N |\n","| Sparsity | Many N-grams rarely appear |\n","| Limited context | Cannot capture long-range dependencies |\n","\n","This led to advanced models like **RNNs â†’ LSTMs â†’ Transformers (BERT, GPT)**.\n","\n"],"metadata":{"id":"oipPz6Cf0hwU"}},{"cell_type":"code","source":["# N-gram implementation using CountVectorizer in sklearn\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Sample documents (example corpus)\n","documents = [\n","    \"I love natural language processing\",\n","    \"Language processing helps computers understand text\",\n","    \"I love deep learning and machine learning\"\n","]\n","\n","# Create CountVectorizer for unigrams + bigrams (1-grams and 2-grams)\n","vectorizer = CountVectorizer(ngram_range=(1, 2))   # (min_n, max_n)\n","\n","# Fit and transform the documents into an N-gram matrix\n","ngram_matrix = vectorizer.fit_transform(documents)\n","\n","# Display vocabulary (token -> index)\n","print(\"Vocabulary (N-grams mapping):\")\n","print(vectorizer.vocabulary_)\n","print(\"\\n\")\n","\n","# Display N-gram matrix\n","print(\"N-gram Matrix Representation:\")\n","print(ngram_matrix.toarray())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RUNt_K9K0g0T","executionInfo":{"status":"ok","timestamp":1765295980051,"user_tz":-330,"elapsed":64,"user":{"displayName":"6676 Aditya","userId":"06090099644586545117"}},"outputId":"a9fe8c7e-c8f3-410e-cc9a-1b352eb17952"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary (N-grams mapping):\n","{'love': 12, 'natural': 17, 'language': 8, 'processing': 19, 'love natural': 14, 'natural language': 18, 'language processing': 9, 'helps': 6, 'computers': 2, 'understand': 22, 'text': 21, 'processing helps': 20, 'helps computers': 7, 'computers understand': 3, 'understand text': 23, 'deep': 4, 'learning': 10, 'and': 0, 'machine': 15, 'love deep': 13, 'deep learning': 5, 'learning and': 11, 'and machine': 1, 'machine learning': 16}\n","\n","\n","N-gram Matrix Representation:\n","[[0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0]\n"," [0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1]\n"," [1 1 0 0 1 1 0 0 0 0 2 1 1 1 0 1 1 0 0 0 0 0 0 0]]\n"]}]},{"cell_type":"markdown","source":["## **Term Frequency â€“ Inverse Document Frequency**"],"metadata":{"id":"RFTIPabv2VoF"}},{"cell_type":"markdown","source":["\n","It is a numerical measure used to evaluate **how important a word is in a document relative to a collection of documents (corpus)**.\n","\n","Unlike Bag of Words, TF-IDF does not simply count word occurrences.  \n","It increases the weight of **important and rare words** and decreases the weight of **common words** (which have low predictive value).\n","\n","---\n","\n","## Intuition\n","Common words such as \"the\", \"is\", and \"of\" appear frequently in most documents and carry little meaning.  \n","TF-IDF identifies words that are **unique and relevant** to a specific document.\n","\n","Example intuition:\n","- In a corpus of news articles, the word **\"inflation\"** will have higher importance in a financial article than in a sports article.\n","\n","---\n","\n","## Mathematical Formulation\n","\n","### Term Frequency (TF)\n","Measures how frequently a term appears within a document.\n","\n","$$\n","TF(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}\n","$$\n","\n","---\n","\n","### Inverse Document Frequency (IDF)\n","Measures how rare a term is across the corpus.\n","\n","Let:\n","- \\(N\\) = total number of documents\n","- \\(df(t)\\) = number of documents containing term \\(t\\)\n","\n","$$\n","IDF(t) = \\log \\left(\\frac{N}{df(t)} \\right)\n","$$\n","\n","---\n","\n","### TF-IDF Score\n","The final score is the product of TF and IDF.\n","\n","$$\n","TFIDF(t, d) = TF(t, d) \\times IDF(t)\n","$$\n","\n","---\n","\n","## Interpretation\n","- A term receives a **high TF-IDF value** if\n","  - it occurs frequently in a document\n","  - it does not appear in many other documents\n","- A term receives a **low TF-IDF value** if\n","  - it occurs in many documents (common words)\n","  - or appears rarely within a specific document\n","\n","---\n","\n","## Geometric / Vector Space Intuition\n","TF-IDF transforms each document into a vector in a high-dimensional feature space:\n","\n","$$\n","D = [tfidf(w_1), tfidf(w_2), \\dots, tfidf(w_V)]\n","$$\n","\n","Similarity between documents can be measured using cosine similarity:\n","\n","$$\n","\\text{Similarity}(D_1, D_2) = \\frac{D_1 \\cdot D_2}{\\|D_1\\| \\|D_2\\|}\n","$$\n","\n","Documents that share important terms become closer in this vector space.\n","\n","---\n","\n","## Advantages\n","- Reduces the influence of common, less meaningful words\n","- Highlights keywords that differentiate documents\n","- Improves classification, clustering, and search relevance\n","\n","---\n","\n","## Limitations\n","- High dimensional and sparse for large vocabularies\n","- Still does not capture long-range semantic meaning\n","- Treats each word independently without contextual information\n","\n","---\n"],"metadata":{"id":"SdDav1TY2dQP"}},{"cell_type":"code","source":["# TF-IDF implementation using sklearn\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Example documents (corpus)\n","documents = [\n","    \"I love natural language processing\",\n","    \"Language processing helps computers understand text\",\n","    \"I love deep learning and machine learning\"\n","]\n","\n","# Initialize TF-IDF Vectorizer\n","tfidf = TfidfVectorizer()\n","\n","# Fit and transform documents into TF-IDF matrix\n","tfidf_matrix = tfidf.fit_transform(documents)\n","\n","# Display vocabulary (word -> index)\n","print(\"Vocabulary (word -> index):\")\n","print(tfidf.vocabulary_)\n","print(\"\\n\")\n","\n","# Display TF-IDF matrix values as array\n","print(\"TF-IDF Matrix:\")\n","print(tfidf_matrix.toarray())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZIYfWUX71vx8","executionInfo":{"status":"ok","timestamp":1765296861161,"user_tz":-330,"elapsed":85,"user":{"displayName":"6676 Aditya","userId":"06090099644586545117"}},"outputId":"d1a02632-f272-4685-f8a1-e95d17e9d52d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary (word -> index):\n","{'love': 6, 'natural': 8, 'language': 4, 'processing': 9, 'helps': 3, 'computers': 1, 'understand': 11, 'text': 10, 'deep': 2, 'learning': 5, 'and': 0, 'machine': 7}\n","\n","\n","TF-IDF Matrix:\n","[[0.         0.         0.         0.         0.45985353 0.\n","  0.45985353 0.         0.60465213 0.45985353 0.         0.        ]\n"," [0.         0.44036207 0.         0.44036207 0.3349067  0.\n","  0.         0.         0.         0.3349067  0.44036207 0.44036207]\n"," [0.36325471 0.         0.36325471 0.         0.         0.72650942\n","  0.27626457 0.36325471 0.         0.         0.         0.        ]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"OtcOmH2A5G5P"},"execution_count":null,"outputs":[]}]}