{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzru5ukUnDYKJM9hFOSxQ9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditya0589/notebooks/blob/main/AI%20Engineering/Langchain/LC03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LC03 LANGCHAIN PROMPTS**"
      ],
      "metadata": {
        "id": "R8-bMBWgN6zf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompts are the input instructions or queries given to a model to guide its output.\n",
        "\n",
        "There are two kinds of ways you can prompt an LLM:\n",
        "1. Static prompts\n",
        "2. Dynamic Prompts\n",
        "\n",
        "Static prompts are not much used in real projects where we want to give a consistant response to the user. Since the model output depends heavily on the prompt, we must ensure that the kind of prompt which the user enters remains in our control. This way we can ensure the quality of outputs given by our system remains consistant"
      ],
      "metadata": {
        "id": "oIGqjk3Q-mA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community langchain_groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "japif-AXFvul",
        "outputId": "032ab1e5-0730-4f60-b9a3-3bcc5f285399"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.3)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-1.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.6)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain_community)\n",
            "  Downloading langchain_classic-1.0.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.45)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain_community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.6.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
            "Collecting groq<1.0.0,>=0.30.0 (from langchain_groq)\n",
            "  Downloading groq-0.37.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain_groq) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain_groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain_groq) (4.15.0)\n",
            "Collecting langchain-text-splitters<2.0.0,>=1.1.0 (from langchain-classic<2.0.0,>=1.0.0->langchain_community)\n",
            "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (25.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.13.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.1)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.3.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain_groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain_groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_groq-1.1.1-py3-none-any.whl (19 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading groq-0.37.1-py3-none-any.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_classic-1.0.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
            "Downloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, mypy-extensions, marshmallow, typing-inspect, groq, dataclasses-json, langchain-text-splitters, langchain_groq, langchain-classic, langchain_community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 groq-0.37.1 langchain-classic-1.0.1 langchain-text-splitters-1.1.0 langchain_community-0.4.1 langchain_groq-1.1.1 marshmallow-3.26.2 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load keys from Colab Secrets\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "# Sanity check (safe partial print)\n",
        "print(\"Groq:\", os.environ[\"GROQ_API_KEY\"][:8])\n",
        "print(\"Google:\", os.environ[\"GOOGLE_API_KEY\"][:8])\n",
        "print('HF_TOKEN', os.environ['HF_TOKEN'][:8])\n",
        "print('huggingface api token', os.environ['HUGGINGFACEHUB_API_TOKEN'][:8])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3M8GfIJGns2",
        "outputId": "ffb85279-85da-443f-a5f6-70346d8ff728"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Groq: gsk_L243\n",
            "Google: AIzaSyCX\n",
            "HF_TOKEN AIzaSyBE\n",
            "huggingface api token hf_SMPRD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YL5NGomzN4ka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e9231c9-8ca2-4f64-fdf8-e28f7c1617d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the prompt: summarize attention is all you need\n",
            "\"Attention is All You Need\" is a research paper published in 2017 by Vaswani et al., which introduced the Transformer model, a groundbreaking architecture for natural language processing (NLP) tasks. Here's a summary of the paper:\n",
            "\n",
            "**Motivation:**\n",
            "\n",
            "1. Traditional sequence-to-sequence models, such as recurrent neural networks (RNNs) and long short-term memory (LSTM) networks, relied on recurrent connections and sequential processing, which can be computationally expensive and prone to vanishing gradients.\n",
            "2. The paper aimed to design a new architecture that could efficiently process long-range dependencies in sequential data, such as sentences and paragraphs.\n",
            "\n",
            "**Key contributions:**\n",
            "\n",
            "1. **Self-Attention Mechanism:** The Transformer model introduced a self-attention mechanism, which allows the model to attend to all positions in the input sequence simultaneously and weigh their importance. This mechanism is computationally efficient and can capture long-range dependencies.\n",
            "2. **Encoder-Decoder Architecture:** The Transformer model consists of an encoder and a decoder. The encoder processes the input sequence and generates a contextualized representation, while the decoder generates the output sequence based on this representation.\n",
            "3. **Multi-Head Attention:** To further improve the model's ability to capture different types of contextual relationships, the paper introduced a multi-head attention mechanism, which allows the model to attend to different aspects of the input sequence in parallel.\n",
            "\n",
            "**Architecture:**\n",
            "\n",
            "1. **Encoder:** The encoder consists of a stack of identical layers, each of which consists of two sub-layers: self-attention and feed-forward neural networks (FFNNs).\n",
            "2. **Decoder:** The decoder also consists of a stack of identical layers, each of which consists of three sub-layers: self-attention, multi-head attention, and FFNNs.\n",
            "\n",
            "**Training:**\n",
            "\n",
            "1. **Masked Language Modeling:** The paper proposed a training objective based on masked language modeling, where a portion of the input sequence is randomly masked and the model is trained to predict the corresponding tokens.\n",
            "2. **Next Sentence Prediction:** The paper also proposed an additional training objective based on next sentence prediction, where the model is trained to predict whether two input sentences are adjacent in the original text.\n",
            "\n",
            "**Results:**\n",
            "\n",
            "1. **English Translation:** The paper reported state-of-the-art results on the WMT'14 English-to-German translation task, achieving a BLEU score of 28.4, which was a significant improvement over the previous best result.\n",
            "2. **Other NLP Tasks:** The paper also reported competitive results on other NLP tasks, such as question answering and text classification.\n",
            "\n",
            "**Conclusion:**\n",
            "\n",
            "The \"Attention is All You Need\" paper introduced a revolutionary architecture for NLP tasks, which has since become a standard building block in many NLP models. The Transformer model has been widely adopted and has achieved state-of-the-art results on many NLP tasks.\n"
          ]
        }
      ],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(model_name=\"llama-3.1-8b-instant\")\n",
        "prompt = input(\"Enter the prompt: \")\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above is an application where we allow the user to enter static prompts. Here we cannot control the consistancy of the responses as we are giving full control to the user regarding the prompts"
      ],
      "metadata": {
        "id": "ueWQAcFnHAde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In dynamic prompts, we developers write the prompt beforehand (not the user). The user only provides us only the key values which he needs."
      ],
      "metadata": {
        "id": "n64BZvr7HnrO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below is an example of how you do it:"
      ],
      "metadata": {
        "id": "MXfr8hWsJJFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(model_name=\"llama-3.1-8b-instant\")\n",
        "name = input(\"Enter the name of the paper you want to summarize: \")\n",
        "prompt = f\"Summarize the paper {name}. Give detailed response with equations and code snippets wherever needed\"\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7vOM79WHdq-",
        "outputId": "004de02d-c6dc-44c6-cf09-8ca2b2758ec0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the name of the paper you want to summarize: attention is all you need\n",
            "**Attention is All You Need**\n",
            "\n",
            "The paper \"Attention is All You Need\" by Vaswani et al. (2017) introduced the Transformer model, a groundbreaking neural network architecture that has revolutionized the field of natural language processing (NLP). The paper presents a new approach to sequence-to-sequence learning that relies entirely on self-attention mechanisms, abandoning the traditional recurrent neural network (RNN) and convolutional neural network (CNN) architectures.\n",
            "\n",
            "**Background**\n",
            "\n",
            "Traditional sequence-to-sequence models, such as RNNs and CNNs, rely on recurrent or convolutional layers to capture temporal dependencies in sequential data. However, these models suffer from several limitations:\n",
            "\n",
            "1. **Sequential computation**: RNNs and CNNs process sequences sequentially, which can lead to slow computation and difficulty in parallelizing the computation.\n",
            "2. **Fixed context window**: RNNs have a fixed context window, which can limit the amount of context that can be captured.\n",
            "3. **Sequential dependency**: RNNs are prone to sequential dependencies, which can lead to vanishing or exploding gradients during training.\n",
            "\n",
            "**Self-Attention Mechanism**\n",
            "\n",
            "The self-attention mechanism is a key component of the Transformer model. It allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. The self-attention mechanism consists of three main components:\n",
            "\n",
            "1. **Query (Q)**: The query is a vector that is used to compute the attention weights.\n",
            "2. **Key (K)**: The key is a vector that is used to compute the attention weights.\n",
            "3. **Value (V)**: The value is a vector that is used to compute the output.\n",
            "\n",
            "The self-attention mechanism can be computed as follows:\n",
            "\n",
            "**Equation 1**: Self-Attention Mechanism\n",
            "\\[ \\text{Attention}(Q, K, V) = \\frac{\\exp(QK^T / \\sqrt{d})}{\\sum_{j=1}^{n} \\exp(QK_j^T / \\sqrt{d})} V \\]\n",
            "\n",
            "where Q, K, V are the query, key, and value vectors, respectively, and d is the dimensionality of the vectors.\n",
            "\n",
            "**Multi-Head Attention**\n",
            "\n",
            "The self-attention mechanism is extended to multi-head attention by concatenating multiple self-attention mechanisms with different weights. This allows the model to capture different aspects of the input sequence simultaneously.\n",
            "\n",
            "**Equation 2**: Multi-Head Attention\n",
            "\\[ \\text{MultiHead}(Q, K, V) = \\text{Concat}(h_1, \\ldots, h_H) W^O \\]\n",
            "\n",
            "where \\(h_1, \\ldots, h_H\\) are the outputs of H different self-attention mechanisms, and W^O is a linear projection matrix.\n",
            "\n",
            "**Encoder and Decoder**\n",
            "\n",
            "The Transformer model consists of an encoder and a decoder. The encoder takes in a sequence of tokens and outputs a sequence of vectors that represent the input sequence. The decoder takes in a sequence of tokens and outputs a sequence of vectors that represent the output sequence.\n",
            "\n",
            "**Code Snippet**\n",
            "```python\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "\n",
            "class SelfAttention(nn.Module):\n",
            "    def __init__(self, hidden_size, num_heads):\n",
            "        super(SelfAttention, self).__init__()\n",
            "        self.query_linear = nn.Linear(hidden_size, hidden_size)\n",
            "        self.key_linear = nn.Linear(hidden_size, hidden_size)\n",
            "        self.value_linear = nn.Linear(hidden_size, hidden_size)\n",
            "        self.dropout = nn.Dropout(0.1)\n",
            "        self.num_heads = num_heads\n",
            "\n",
            "    def forward(self, input_seq):\n",
            "        q = self.query_linear(input_seq)\n",
            "        k = self.key_linear(input_seq)\n",
            "        v = self.value_linear(input_seq)\n",
            "\n",
            "        attention_weights = torch.matmul(q, k.T) / math.sqrt(q.shape[-1])\n",
            "        attention_weights = self.dropout(torch.nn.functional.softmax(attention_weights, dim=-1))\n",
            "        output = torch.matmul(attention_weights, v)\n",
            "        return output\n",
            "\n",
            "class MultiHeadAttention(nn.Module):\n",
            "    def __init__(self, hidden_size, num_heads):\n",
            "        super(MultiHeadAttention, self).__init__()\n",
            "        self.self_attention = SelfAttention(hidden_size, num_heads)\n",
            "        self.linear_projection = nn.Linear(hidden_size * num_heads, hidden_size)\n",
            "\n",
            "    def forward(self, input_seq):\n",
            "        output = self.self_attention(input_seq)\n",
            "        output = torch.reshape(output, (-1, self.self_attention.num_heads, output.shape[-1] // self.self_attention.num_heads))\n",
            "        output = torch.transpose(output, 0, 1)\n",
            "        output = self.linear_projection(torch.reshape(output, (-1, output.shape[-1])))\n",
            "        return output\n",
            "```\n",
            "**Transformer Model**\n",
            "\n",
            "The Transformer model consists of an encoder and a decoder. The encoder takes in a sequence of tokens and outputs a sequence of vectors that represent the input sequence. The decoder takes in a sequence of tokens and outputs a sequence of vectors that represent the output sequence.\n",
            "\n",
            "**Code Snippet**\n",
            "```python\n",
            "class Transformer(nn.Module):\n",
            "    def __init__(self, hidden_size, num_heads, num_layers, dropout):\n",
            "        super(Transformer, self).__init__()\n",
            "        self.encoder = nn.ModuleList([MultiHeadAttention(hidden_size, num_heads) for _ in range(num_layers)])\n",
            "        self.decoder = nn.ModuleList([MultiHeadAttention(hidden_size, num_heads) for _ in range(num_layers)])\n",
            "\n",
            "    def encoder_forward(self, input_seq):\n",
            "        for layer in self.encoder:\n",
            "            input_seq = layer(input_seq)\n",
            "        return input_seq\n",
            "\n",
            "    def decoder_forward(self, input_seq):\n",
            "        for layer in self.decoder:\n",
            "            input_seq = layer(input_seq)\n",
            "        return input_seq\n",
            "\n",
            "# Initialize the model\n",
            "model = Transformer(hidden_size=512, num_heads=8, num_layers=6, dropout=0.1)\n",
            "\n",
            "# Forward pass\n",
            "input_seq = torch.randn(1, 10, 512)\n",
            "output = model.encoder_forward(input_seq)\n",
            "output = model.decoder_forward(output)\n",
            "```\n",
            "The Transformer model has several advantages over traditional sequence-to-sequence models:\n",
            "\n",
            "1. **Parallelization**: The Transformer model can be parallelized more easily than traditional sequence-to-sequence models, which can lead to significant speedups on large datasets.\n",
            "2. **Scalability**: The Transformer model can capture long-range dependencies in sequential data, which makes it suitable for tasks such as machine translation and text summarization.\n",
            "3. **Flexibility**: The Transformer model can be easily adapted to different tasks and datasets, which makes it a versatile tool for NLP applications.\n",
            "\n",
            "However, the Transformer model also has several limitations:\n",
            "\n",
            "1. **Computational cost**: The Transformer model requires a significant amount of computational resources to train and run, which can be a bottleneck for large datasets.\n",
            "2. **Overfitting**: The Transformer model can suffer from overfitting, which can lead to poor performance on test data.\n",
            "3. **Limited interpretability**: The Transformer model can be difficult to interpret, which can make it challenging to understand the underlying mechanisms that drive its performance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find that in the above prompt, we as developers pre-defined the prompt and the user only gives the name of the paper he wants to summarize.\n",
        "\n",
        "This way we were able to control how the output is to be designed in our program"
      ],
      "metadata": {
        "id": "Q3S-lpgdKCad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Langchain PromptTemplate**"
      ],
      "metadata": {
        "id": "Etb5NhA-K2mw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **PromptTemplate** in langchain is a structured way to create prompts dynamically by inserting variables into a predefined template. Instead of hardcoding prompts, PromptTemplate allows you to define placeholders that can be filled in at runtime with different inputs.\n",
        "\n",
        "This makes it reusable, flexible and easy to manage. This makes it easy to work with dynamic user inputs or automatic workflows\n",
        "\n",
        "\n",
        "**Why use PromptTemplate over f strings:**\n",
        "\n",
        "1. **Default Validation:** Langchain automatically validates if all the specified variables in the prompt are initialized.\n",
        "2. **Reusable:** A prompt can be reused by saving it as a JSON.\n",
        "3. **Langchain ecosystem:** Prompt template is havily coupled with the rest of the Langchain ecosystem such as chains and agents."
      ],
      "metadata": {
        "id": "Ek0Twa8YK6Al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "template = PromptTemplate(\n",
        "    template = \"\"\"\n",
        "    Summarize the paper {paper_name} in about\n",
        "    {word_count} words. Add equations and code snippets wherever possible.\n",
        "    \"\"\",\n",
        "    input_variables = [\"paper_name\", \"word_count\"]\n",
        ")\n",
        "\n",
        "prompt = template.invoke(\n",
        "    {\n",
        "        \"paper_name\": \"Attention is all you need\",\n",
        "        \"word_count\": 100\n",
        "    }\n",
        ")\n",
        "\n",
        "llm = ChatGroq(model_name=\"llama-3.1-8b-instant\")\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDHMzssYFsNt",
        "outputId": "2c2ab8a5-fc8e-43d8-cbe5-14be567534c0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**\"Attention is all you need\" Paper Summary**\n",
            "\n",
            "The paper \"Attention is all you need\" by Vaswani et al. (2017) introduced the Transformer model, a revolutionary architecture for sequence-to-sequence tasks. The Transformer replaces traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs) with self-attention mechanisms.\n",
            "\n",
            "**Key Components:**\n",
            "\n",
            "1. **Self-Attention Mechanism**: The Transformer uses a multi-head attention mechanism to compute weighted sums of the input representations.\n",
            "    \\[ \\text{Attention(Q, K, V)} = \\text{softmax}(\\frac{\\text{QK}^T}{\\sqrt{d_k}}) \\text{V} \\]\n",
            "    where Q, K, and V are the query, key, and value matrices, respectively.\n",
            "\n",
            "2. **Encoder-Decoder Architecture**: The Transformer consists of an encoder and a decoder. The encoder takes in a sequence of tokens and outputs a sequence of context vectors. The decoder generates output tokens based on the context vectors.\n",
            "\n",
            "**Code Snippet (PyTorch):**\n",
            "\n",
            "```python\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "\n",
            "class MultiHeadAttention(nn.Module):\n",
            "    def __init__(self, embed_dim, num_heads):\n",
            "        super(MultiHeadAttention, self).__init__()\n",
            "        self.query_linear = nn.Linear(embed_dim, embed_dim)\n",
            "        self.key_linear = nn.Linear(embed_dim, embed_dim)\n",
            "        self.value_linear = nn.Linear(embed_dim, embed_dim)\n",
            "        self.dropout = nn.Dropout(0.1)\n",
            "        self.num_heads = num_heads\n",
            "\n",
            "    def forward(self, query, key, value):\n",
            "        # Compute attention weights\n",
            "        attention_weights = torch.matmul(self.query_linear(query), self.key_linear(key).transpose(-1, -2)) / math.sqrt(self.embed_dim)\n",
            "        attention_weights = self.dropout(torch.softmax(attention_weights, dim=-1))\n",
            "\n",
            "        # Compute weighted sum of value\n",
            "        output = torch.matmul(attention_weights, value)\n",
            "\n",
            "        return output\n",
            "```\n",
            "\n",
            "The Transformer has achieved state-of-the-art results in machine translation, text summarization, and other sequence-to-sequence tasks, outperforming traditional RNNs and CNNs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose the user forgets to add an input value, we can set a validation parameter to handle it:"
      ],
      "metadata": {
        "id": "S3jsdUbvvgwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "template = PromptTemplate(\n",
        "    template = \"\"\"\n",
        "    Summarize the paper {paper_name} in about\n",
        "    {word_count} words. Add equations and code snippets wherever possible.\n",
        "    \"\"\",\n",
        "    input_variables = [\"paper_name\"],\n",
        "    validate_template = True\n",
        ")\n",
        "\n",
        "prompt = template.invoke(\n",
        "    {\n",
        "        \"paper_name\": \"Attention is all you need\",\n",
        "        \"word_count\": 100\n",
        "    }\n",
        ")\n",
        "\n",
        "llm = ChatGroq(model_name=\"llama-3.1-8b-instant\")\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "Tgcu8RXAqNr4",
        "outputId": "03312542-c277-49dc-b7fb-10380d06242a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "1 validation error for PromptTemplate\n  Value error, Invalid prompt schema; check for mismatched or missing input parameters from ['paper_name']. [type=value_error, input_value={'template': '\\n    Summa...'partial_variables': {}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/value_error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1590128174.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_groq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatGroq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m template = PromptTemplate(\n\u001b[0m\u001b[1;32m      5\u001b[0m     template = \"\"\"\n\u001b[1;32m      6\u001b[0m     \u001b[0mSummarize\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpaper\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mpaper_name\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mabout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m  \u001b[0;31m# noqa: D419  # Intentional blank docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mvalidated_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidated_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for PromptTemplate\n  Value error, Invalid prompt schema; check for mismatched or missing input parameters from ['paper_name']. [type=value_error, input_value={'template': '\\n    Summa...'partial_variables': {}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/value_error"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find that this returns a `` validation error``. This is because we forgot to specify an input variable in the list (there are two input variables in the prompt, but only one specified in the list)"
      ],
      "metadata": {
        "id": "0oGxjUfJwKTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can save the template as a JSON object for future reuse as many times as we want"
      ],
      "metadata": {
        "id": "jQJBnYSi0G5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template.save('template.json')"
      ],
      "metadata": {
        "id": "k3wfNIiTvxN0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "RbfTswy30ODN",
        "outputId": "278f2c40-2f06-43f4-f62f-87fc72bf76ed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method BaseModel.json of PromptTemplate(input_variables=['paper_name', 'word_count'], input_types={}, partial_variables={}, template='\\n    Summarize the paper {paper_name} in about \\n    {word_count} words. Add equations and code snippets wherever possible.\\n    ')>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pydantic.main.BaseModel.json</b><br/>def json(*, include: IncEx | None=None, exclude: IncEx | None=None, by_alias: bool=False, exclude_unset: bool=False, exclude_defaults: bool=False, exclude_none: bool=False, encoder: Callable[[Any], Any] | None=PydanticUndefined, models_as_dict: bool=PydanticUndefined, **dumps_kwargs: Any) -&gt; str</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/pydantic/main.py</a>&lt;no docstring&gt;</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 1328);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code snippet below, we are loading the json file to access the prompt template"
      ],
      "metadata": {
        "id": "nGGOcDSv05Qg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate, load_prompt\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "template = load_prompt('template.json')\n",
        "\n",
        "prompt = template.invoke(\n",
        "    {\n",
        "        \"paper_name\": \"Attention is all you need\",\n",
        "        \"word_count\": 100\n",
        "    }\n",
        ")\n",
        "\n",
        "llm = ChatGroq(model_name=\"llama-3.1-8b-instant\")\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IePRXYJs0Qt4",
        "outputId": "9d4ec1f5-1b66-4cbf-8045-c5b787af852f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Summary of \"Attention is All You Need\" Paper**\n",
            "\n",
            "The paper \"Attention is All You Need\" by Vaswani et al. (2017) introduces the Transformer model, a novel architecture for sequence-to-sequence tasks. The Transformer model uses self-attention mechanisms to weigh the importance of different input elements, eliminating the need for recurrent neural networks (RNNs) or convolutional neural networks (CNNs).\n",
            "\n",
            "**Self-Attention Mechanism:**\n",
            "\n",
            "Given a sequence of input vectors `x1, x2, ..., xn`, the self-attention mechanism computes a weighted sum of these vectors. The weights are computed as:\n",
            "\n",
            "`Attention(Q, K, V) = softmax(QK^T/sqrt(d))V`\n",
            "\n",
            "where `Q` (query), `K` (key), and `V` (value) are learned linear transformations of the input vectors.\n",
            "\n",
            "**Multi-Head Attention:**\n",
            "\n",
            "The Transformer model uses multi-head attention to jointly attend to information from different representation subspaces at different positions. This is achieved by applying attention in parallel multiple times, with different learned linear projections:\n",
            "\n",
            "`MultiHead(Q, K, V) = Concat(head1, ..., headh)W^O`\n",
            "\n",
            "where `headi = Attention(Qi, Ki, Vi)`\n",
            "\n",
            "**Transformer Encoder:**\n",
            "\n",
            "The Transformer encoder is composed of a stack of identical layers, each consisting of two sub-layers:\n",
            "\n",
            "1. Multi-Head Self-Attention:\n",
            "```python\n",
            "class MultiHeadAttention(nn.Module):\n",
            "    def __init__(self, d_model, num_heads):\n",
            "        super(MultiHeadAttention, self).__init__()\n",
            "        self.query_linear = nn.Linear(d_model, d_model)\n",
            "        self.key_linear = nn.Linear(d_model, d_model)\n",
            "        self.value_linear = nn.Linear(d_model, d_model)\n",
            "        self.dropout = nn.Dropout(p=0.1)\n",
            "        self.num_heads = num_heads\n",
            "\n",
            "    def forward(self, x):\n",
            "        Q = self.query_linear(x)\n",
            "        K = self.key_linear(x)\n",
            "        V = self.value_linear(x)\n",
            "        attention_weights = self.dropout(Q * K.T / math.sqrt(Q.shape[-1]))\n",
            "        context = attention_weights * V\n",
            "        return context\n",
            "```\n",
            "\n",
            "2. Feed Forward Network (FFN):\n",
            "```python\n",
            "class FeedForwardNetwork(nn.Module):\n",
            "    def __init__(self, d_model, d_ff):\n",
            "        super(FeedForwardNetwork, self).__init__()\n",
            "        self.linear1 = nn.Linear(d_model, d_ff)\n",
            "        self.dropout = nn.Dropout(p=0.1)\n",
            "        self.linear2 = nn.Linear(d_ff, d_model)\n",
            "\n",
            "    def forward(self, x):\n",
            "        x = self.dropout(torch.relu(self.linear1(x)))\n",
            "        x = self.linear2(x)\n",
            "        return x\n",
            "```\n",
            "\n",
            "**Transformer Decoder:**\n",
            "\n",
            "The Transformer decoder is similar to the encoder, but with an additional layer for outputting the final sequence.\n",
            "\n",
            "**Equations:**\n",
            "\n",
            "The Transformer model is trained to minimize the following loss function:\n",
            "\n",
            "`L = ∑[ytrue(y) - ypred(y)]`\n",
            "\n",
            "where `ytrue(y)` is the true output and `ypred(y)` is the predicted output.\n",
            "\n",
            "The Transformer model has several advantages over traditional sequence-to-sequence models, including:\n",
            "\n",
            "* **Parallelization:** The Transformer model can be parallelized more easily than RNNs or CNNs.\n",
            "* **Efficiency:** The Transformer model is more efficient than RNNs or CNNs, requiring less computation and memory.\n",
            "* **Flexibility:** The Transformer model can be used for a wide range of sequence-to-sequence tasks, including machine translation, text summarization, and image captioning.\n",
            "\n",
            "Overall, the Transformer model is a powerful and efficient architecture for sequence-to-sequence tasks, and has achieved state-of-the-art results in many applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Langchain Messages**"
      ],
      "metadata": {
        "id": "r33XR871KpB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "model = ChatGroq(model_name=\"llama-3.1-8b-instant\")\n",
        "\n",
        "print(\"Chat. Type 'exit' to break\")\n",
        "while True:\n",
        "  user_input = input(\"You: \")\n",
        "  if user_input == 'exit':\n",
        "    break\n",
        "  response = model.invoke(user_input)\n",
        "  print(\"Chat: \", response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uv4g7A-T0lYb",
        "outputId": "2778d535-d629-473a-aacb-6ddcba852cb3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chat. Type 'exit' to break\n",
            "You: hello\n",
            "Chat:  Hello. How can I assist you today?\n",
            "You: what is my name?\n",
            "Chat:  I don't have any information about your name. I'm a large language model, I don't have personal interactions or memories, so I don't know your name unless you tell me. If you'd like to share your name, I'd be happy to chat with you!\n",
            "You: sing me a song\n",
            "Chat:  (Verse 1)\n",
            "In a world of dreams, where stars are bright\n",
            "A melody whispers through the night\n",
            "A gentle breeze that rustles through the trees\n",
            "A lullaby that only the heart can see\n",
            "\n",
            "(Chorus)\n",
            "Oh, the moon is high, the wind is low\n",
            "The shadows dance, and the night grows slow\n",
            "In this moment, I am free to roam\n",
            "Where the music takes me, I am never alone\n",
            "\n",
            "(Verse 2)\n",
            "In a sea of clouds, where the sun dips low\n",
            "A symphony of colors starts to grow\n",
            "A canvas painted with hues of gold\n",
            "A masterpiece that's yet to be told\n",
            "\n",
            "(Chorus)\n",
            "Oh, the moon is high, the wind is low\n",
            "The shadows dance, and the night grows slow\n",
            "In this moment, I am free to roam\n",
            "Where the music takes me, I am never alone\n",
            "\n",
            "(Bridge)\n",
            "In the silence, I hear a voice\n",
            "A whisper in my ear, a heartfelt choice\n",
            "To follow the rhythm, to let go of fear\n",
            "To find my way, to dry my tears\n",
            "\n",
            "(Chorus)\n",
            "Oh, the moon is high, the wind is low\n",
            "The shadows dance, and the night grows slow\n",
            "In this moment, I am free to roam\n",
            "Where the music takes me, I am never alone\n",
            "\n",
            "(Outro)\n",
            "In this world of dreams, where stars are bright\n",
            "The melody whispers through the night\n",
            "A gentle breeze that rustles through the trees\n",
            "A lullaby that only the heart can see.\n",
            "You: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might find out that the above chatbot does not have any memory ie: the chatbot has no memory of the previous questions asked, it only takes a question and gives the response to the current question. If we ask it about something already mentioned previously in the chat, it has no memory of it."
      ],
      "metadata": {
        "id": "isbE3eW7YgLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can solve this problem by maintaining the chat history in the form of an array and then passing this vhat history to the model each time.\n",
        "\n",
        "This way the model recieves the previous messages along with the recent message and can answer accordingly."
      ],
      "metadata": {
        "id": "qLniME_ufVAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "model = ChatGroq(model_name=\"llama-3.1-8b-instant\")\n",
        "\n",
        "chat_history = []\n",
        "print(\"Chat. Type 'exit' to break\")\n",
        "while True:\n",
        "  user_input = input(\"You: \")\n",
        "  chat_history.append(user_input)\n",
        "  if user_input == 'exit':\n",
        "    break\n",
        "  response = model.invoke(chat_history)\n",
        "  chat_history.append(response.content)\n",
        "  print(\"Chat: \", response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb1u4S10MDc9",
        "outputId": "6125cec3-685b-41e4-9f3f-8795640ed14e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chat. Type 'exit' to break\n",
            "You: hello\n",
            "Chat:  Hello, how can I assist you today?\n",
            "You: what is my name?\n",
            "Chat:  I don't have any information about your name. This is the beginning of our conversation, and I don't retain any data about individual users. If you'd like to share your name with me, I'm happy to chat with you and address you by it.\n",
            "You: my name is aditya\n",
            "Chat:  It's nice to meet you, Aditya. I don't retain any information about our conversation, so feel free to start fresh and ask me anything you'd like. What's on your mind today?\n",
            "You: what is my name\n",
            "Chat:  We've been here before, Aditya. I told you earlier that I don't retain any information, so I won't be able to recall that your name is Aditya. However, you've just told me, so I'm happy to chat with you as Aditya now.\n",
            "You: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can find that now the model is able to remember the previous conversation.\n",
        "\n",
        "If we now print the chat history, we find that its a single list with the messages. But we are not able to store the information about which messages is said by whom"
      ],
      "metadata": {
        "id": "ZSqzsh7EgFou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dZcznf1f7hz",
        "outputId": "20951ccc-250b-4156-8c12-82eebe12c49c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'Hello, how can I assist you today?', 'what is my name?', \"I don't have any information about your name. This is the beginning of our conversation, and I don't retain any data about individual users. If you'd like to share your name with me, I'm happy to chat with you and address you by it.\", 'my name is aditya', \"It's nice to meet you, Aditya. I don't retain any information about our conversation, so feel free to start fresh and ask me anything you'd like. What's on your mind today?\", 'what is my name', \"We've been here before, Aditya. I told you earlier that I don't retain any information, so I won't be able to recall that your name is Aditya. However, you've just told me, so I'm happy to chat with you as Aditya now.\", 'exit']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to solve this problem, we need to use a dictionary instead of a list. This dictionary should not only store the messages but also who said those messages\n",
        "\n",
        "like:\n",
        "\n",
        "You: hi\n",
        "\n",
        "AI: Hello\n",
        "\n",
        "and so on"
      ],
      "metadata": {
        "id": "HM9O4h3hhbj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Langchain messages helps you achieve this. You dont need to manually implement these data structures.\n",
        "\n",
        "Lanchain has three types of messages:\n",
        "1. System message\n",
        "2. Human message\n",
        "3. AI message\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m0kW8AoKhs_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "model = ChatGroq(model_name=\"llama-3.1-8b-instant\")\n",
        "\n",
        "messages=[\n",
        "    SystemMessage(content='You are a helpful assistant'),\n",
        "    HumanMessage(content='Tell me about LangChain')\n",
        "]\n",
        "\n",
        "result = model.invoke(messages)\n",
        "print(result.content)\n",
        "\n",
        "messages.append(AIMessage(content=result.content))\n",
        "\n",
        "print(messages)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhD5fvCJhaZC",
        "outputId": "a4132913-90ed-4007-d6fe-7ef677fc3078"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain is an open-source Python library for building large language models and AI systems. It was developed by Llama.com, a company that provides a platform for building and deploying AI models.\n",
            "\n",
            "LangChain provides a set of tools and APIs that make it easier to build and integrate large language models into various applications. Some of the key features of LangChain include:\n",
            "\n",
            "1. **Model Integration**: LangChain allows developers to easily integrate large language models, such as LLaMA, BART, and T5, into their applications.\n",
            "2. **Conversational Interfaces**: LangChain provides tools for building conversational interfaces, such as chatbots and voice assistants, using large language models.\n",
            "3. **Text Generation**: LangChain allows developers to generate text programmatically using large language models.\n",
            "4. **Text Classification**: LangChain provides tools for text classification tasks, such as sentiment analysis and topic modeling.\n",
            "5. **Dialogue Management**: LangChain provides tools for managing dialogue flows and conversations.\n",
            "\n",
            "Some of the benefits of using LangChain include:\n",
            "\n",
            "1. **Easy Model Integration**: LangChain makes it easy to integrate large language models into applications, without requiring extensive knowledge of machine learning or natural language processing.\n",
            "2. **Flexibility**: LangChain provides a range of tools and APIs that can be used to build a variety of AI applications, from chatbots to text generators.\n",
            "3. **Scalability**: LangChain is designed to handle large volumes of data and can be scaled to meet the needs of complex AI applications.\n",
            "\n",
            "Some of the use cases for LangChain include:\n",
            "\n",
            "1. **Chatbots**: LangChain can be used to build conversational interfaces, such as chatbots and voice assistants.\n",
            "2. **Virtual Assistants**: LangChain can be used to build virtual assistants that can perform a range of tasks, from answering questions to generating text.\n",
            "3. **Content Generation**: LangChain can be used to generate high-quality content, such as articles and product descriptions.\n",
            "4. **Customer Service**: LangChain can be used to build chatbots and virtual assistants that can help customers with support and service requests.\n",
            "\n",
            "Overall, LangChain is a powerful tool for building AI applications that use large language models. Its ease of use, flexibility, and scalability make it a popular choice among developers and organizations looking to build conversational interfaces and generate high-quality content.\n",
            "[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about LangChain', additional_kwargs={}, response_metadata={}), AIMessage(content='LangChain is an open-source Python library for building large language models and AI systems. It was developed by Llama.com, a company that provides a platform for building and deploying AI models.\\n\\nLangChain provides a set of tools and APIs that make it easier to build and integrate large language models into various applications. Some of the key features of LangChain include:\\n\\n1. **Model Integration**: LangChain allows developers to easily integrate large language models, such as LLaMA, BART, and T5, into their applications.\\n2. **Conversational Interfaces**: LangChain provides tools for building conversational interfaces, such as chatbots and voice assistants, using large language models.\\n3. **Text Generation**: LangChain allows developers to generate text programmatically using large language models.\\n4. **Text Classification**: LangChain provides tools for text classification tasks, such as sentiment analysis and topic modeling.\\n5. **Dialogue Management**: LangChain provides tools for managing dialogue flows and conversations.\\n\\nSome of the benefits of using LangChain include:\\n\\n1. **Easy Model Integration**: LangChain makes it easy to integrate large language models into applications, without requiring extensive knowledge of machine learning or natural language processing.\\n2. **Flexibility**: LangChain provides a range of tools and APIs that can be used to build a variety of AI applications, from chatbots to text generators.\\n3. **Scalability**: LangChain is designed to handle large volumes of data and can be scaled to meet the needs of complex AI applications.\\n\\nSome of the use cases for LangChain include:\\n\\n1. **Chatbots**: LangChain can be used to build conversational interfaces, such as chatbots and voice assistants.\\n2. **Virtual Assistants**: LangChain can be used to build virtual assistants that can perform a range of tasks, from answering questions to generating text.\\n3. **Content Generation**: LangChain can be used to generate high-quality content, such as articles and product descriptions.\\n4. **Customer Service**: LangChain can be used to build chatbots and virtual assistants that can help customers with support and service requests.\\n\\nOverall, LangChain is a powerful tool for building AI applications that use large language models. Its ease of use, flexibility, and scalability make it a popular choice among developers and organizations looking to build conversational interfaces and generate high-quality content.', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above is the basic implementation of the langchain messages. we can see how the different type of messages work"
      ],
      "metadata": {
        "id": "q6we4Ci0kkWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now use this messages concept in our chatbot"
      ],
      "metadata": {
        "id": "1JalvY6Lkw5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "model = ChatGroq(model_name=\"llama-3.1-8b-instant\")\n",
        "\n",
        "chat_history = [\n",
        "    SystemMessage(content='You are a helpful AI assistant')\n",
        "]\n",
        "\n",
        "while True:\n",
        "    user_input = input('You: ')\n",
        "    chat_history.append(HumanMessage(content=user_input))\n",
        "    if user_input == 'exit':\n",
        "        break\n",
        "    result = model.invoke(chat_history)\n",
        "    chat_history.append(AIMessage(content=result.content))\n",
        "    print(\"AI: \",result.content)\n",
        "\n",
        "print(chat_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwkxEvNmkNy0",
        "outputId": "f8249641-0cd5-482d-96d9-f73c1e7305c5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: hello\n",
            "AI:  Hello. How can I assist you today?\n",
            "You: what is my name?\n",
            "AI:  I don't have any information about your name. I'm a conversational AI, and our conversation just started. You can tell me your name if you'd like, and I can use it to address you throughout our conversation.\n",
            "You: my name is aditya\n",
            "AI:  Nice to meet you, Aditya. Is there something specific you'd like to talk about or would you like some recommendations on topics I can help you with?\n",
            "You: what did i ask you until now?\n",
            "AI:  You asked me the following:\n",
            "\n",
            "1. \"Hello\"\n",
            "2. \"what is my name?\"\n",
            "3. \"my name is aditya\"\n",
            "4. \"what did i ask you until now?\"\n",
            "You: exit\n",
            "[SystemMessage(content='You are a helpful AI assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='hello', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello. How can I assist you today?', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I don't have any information about your name. I'm a conversational AI, and our conversation just started. You can tell me your name if you'd like, and I can use it to address you throughout our conversation.\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='my name is aditya', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Nice to meet you, Aditya. Is there something specific you'd like to talk about or would you like some recommendations on topics I can help you with?\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='what did i ask you until now?', additional_kwargs={}, response_metadata={}), AIMessage(content='You asked me the following:\\n\\n1. \"Hello\"\\n2. \"what is my name?\"\\n3. \"my name is aditya\"\\n4. \"what did i ask you until now?\"', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='exit', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can find that in the above chat history every message is labelled if its a HumanMessage, a SystemMessage or an AIMessage"
      ],
      "metadata": {
        "id": "VKhBSBnflSqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Chat Prompt Template**\n",
        "\n",
        "\n",
        "A **Chat Prompt Template** is a template used to create prompts specifically for **chat-based models** (like ChatGPT).  \n",
        "It structures the prompt as a **conversation**, using roles such as:\n",
        "- **System** – instructions or rules for the model\n",
        "- **Human** – user messages\n",
        "- **AI** – assistant responses\n",
        "\n",
        "- **Prompt Template** → Used for **single text prompts** (one block of text).\n",
        "- **Chat Prompt Template** → Used for **conversations with roles**.\n",
        "\n",
        "### Simple explanation:\n",
        "> Chat models think in conversations, not plain text.  \n",
        "> Chat Prompt Templates help you talk to them the *right way* by separating instructions, user input, and responses.\n",
        "\n",
        "- Use **PromptTemplate** for simple text generation\n",
        "- Use **ChatPromptTemplate** when working with **chat models** to get better, more accurate responses\n"
      ],
      "metadata": {
        "id": "TzBV5FLiorCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_template = ChatPromptTemplate([\n",
        "    ('system', 'You are a helpful {domain} expert'),\n",
        "    ('human', 'Explain in simple terms, what is {topic}')\n",
        "])\n",
        "\n",
        "prompt = chat_template.invoke({'domain':'cricket','topic':'Dusra'})\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rM9LIm8k_BS",
        "outputId": "3d1a599d-0ff4-4218-aa50-18c0a2860ae5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='You are a helpful cricket expert', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain in simple terms, what is Dusra', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Message Placeholder**\n",
        "\n",
        "A **Message Placeholder** in Langchain is a special placeholder used inside a chatprompt template to dynamically insert chat history on a list of messages during runtime."
      ],
      "metadata": {
        "id": "ws3uAWCppBmn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us assume we have the chat history stored in an external file. We want to insert it dynamically in runtime"
      ],
      "metadata": {
        "id": "5DFEO5Kipxf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "# chat template\n",
        "chat_template = ChatPromptTemplate([\n",
        "    ('system','You are a helpful customer support agent'),\n",
        "    MessagesPlaceholder(variable_name='chat_history'),\n",
        "    ('human','{query}')\n",
        "])\n",
        "\n",
        "chat_history = []\n",
        "# load chat history\n",
        "with open('chat_history.txt') as f:\n",
        "    chat_history.extend(f.readlines())\n",
        "\n",
        "print(chat_history)\n",
        "\n",
        "# create prompt\n",
        "prompt = chat_template.invoke({'chat_history':chat_history, 'query':'Where is my refund'})\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KToqiAx-o7fD",
        "outputId": "efc0c5bb-d452-490e-ce1c-8372aadc1cf2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['HumanMessage(content=\"I want to request a refund for my order #12345.\")\\n', 'AIMessage(content=\"Your refund request for order #12345 has been initiated. It will be processed in 3-5 business days.\")']\n",
            "messages=[SystemMessage(content='You are a helpful customer support agent', additional_kwargs={}, response_metadata={}), HumanMessage(content='HumanMessage(content=\"I want to request a refund for my order #12345.\")\\n', additional_kwargs={}, response_metadata={}), HumanMessage(content='AIMessage(content=\"Your refund request for order #12345 has been initiated. It will be processed in 3-5 business days.\")', additional_kwargs={}, response_metadata={}), HumanMessage(content='Where is my refund', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LXWUBF2apamq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}